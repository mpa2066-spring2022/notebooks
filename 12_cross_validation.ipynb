{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Cross Validation and Hyperparameters</h1></center>\n",
    "<center><h3>Paul Stey</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Cross Validation?\n",
    "- we want to find the best hyper-parameters of our ML algorithms\n",
    "   - fit model to training data (`.fit(X_train,y_train)`)\n",
    "   - evaluate model on CV set (`.predict(X_CV,y_CV)`)\n",
    "   - we find hyper-parameter values that optimize the CV score\n",
    "- we want to know how the model will perform on previously unseen data\n",
    "   - apply our final model on the test set (`.predict(X_test,y_test)`)\n",
    "   \n",
    "- we need to split the data into parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How should we split the data into train/CV/test?\n",
    "\n",
    "- data is **Independent and Identically Distributed** (iid)\n",
    "   - all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples\n",
    "   - identify cats and dogs on images\n",
    "   - predict if someone's salary is above or below 50k\n",
    "- examples of not iid data:\n",
    "   - data generated by time-dependent processes\n",
    "   - data has group structure (samples collected from e.g., different subjects, experiments, measurement devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CV steps of iid data to avoid mistakes \n",
    "- shuffle and split the data\n",
    "- preprocess (fit_transform train, transform the rest)\n",
    "- decide on the evaluation metric\n",
    "- decide ML algo, which hyper-parameters you tune, and what values you want to try\n",
    "- loop over all combinations and save train and CV scores\n",
    "- find best model based on optimal CV score\n",
    "- report test score using the best model\n",
    "- repeat a couple of times with different random states to estimate uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting strategies for iid data: basic approach\n",
    "- the basic aproach:\n",
    "   - 60% train, 20% CV, 20% test \n",
    "   - the ratios can vary somewhat but the training set should contain most of your points\n",
    "   - if you redo the split with a different random state, the results will change\n",
    "      - repeat the split a couple of times to measure model uncertainty due to splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's put everything together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "def true_fun(X):\n",
    "    return 2*X + np.cos(4 * np.pi * X)\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "X = np.random.randn(n_samples)\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, marker = '.')\n",
    "plt.rcParams['figure.figsize'] = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ml_pipeline_basic(X, y, random_state):\n",
    "    # split the data\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)\n",
    "    X_train, X_CV, y_train, y_CV = train_test_split(X_other, y_other, test_size=0.25, random_state = random_state)\n",
    "    \n",
    "    # simple preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_CV = scaler.transform(X_CV)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # tune ridge hyper-parameter, alpha\n",
    "    alpha = np.logspace(-3, 4, num = 8)\n",
    "    train_score = []\n",
    "    CV_score = []\n",
    "    regs = []\n",
    "    \n",
    "    for a in alpha:\n",
    "        reg = Ridge(alpha = a)\n",
    "        reg.fit(X_train,y_train)\n",
    "        train_score.append(mean_squared_error(y_train, reg.predict(X_train)))\n",
    "        CV_score.append(mean_squared_error(y_CV, reg.predict(X_CV)))\n",
    "        regs.append(reg)\n",
    "    \n",
    "    # find the best alpha\n",
    "    idx_best = np.argmin(CV_score)\n",
    "    best_alpha = alpha[idx_best]\n",
    "    \n",
    "    # grab the best model\n",
    "    reg = regs[idx_best]\n",
    "    \n",
    "    # calculate holdout score\n",
    "    test_score = mean_squared_error(y_test,reg.predict(X_test))\n",
    "\n",
    "    return best_alpha, np.min(CV_score), test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CV_scores = []\n",
    "test_scores = []\n",
    "for i in range(10):\n",
    "    best_alpha, CV_score, test_score = ml_pipeline_basic(X[:, np.newaxis], y, i*42 )\n",
    "    CV_scores.append(CV_score)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "\n",
    "print('CV MSE:', np.around(np.mean(CV_scores),2),'+/-',np.around(np.std(CV_scores),2))\n",
    "print('test MSE:', np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting strategies for iid data, k-fold cross validation\n",
    "\n",
    "<center><img src=\"images/grid_search_cross_validation.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ml_pipeline_kfold(X,y,random_state,n_folds):\n",
    "    # split the data\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state)\n",
    "    CV_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    # k folds - each fold will give us a CV and a test score\n",
    "    kf = KFold(n_splits=n_folds, shuffle = True,random_state=random_state)\n",
    "    \n",
    "    for train_index, CV_index in kf.split(X_other,y_other):\n",
    "        X_train, X_CV = X_other[train_index], X_other[CV_index]\n",
    "        y_train, y_CV = y_other[train_index], y_other[CV_index]\n",
    "        \n",
    "        # simple preprocessing\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_c = scaler.transform(X_CV)\n",
    "        X_t = scaler.transform(X_test)\n",
    "        \n",
    "        # tune ridge hyper-parameter, alpha\n",
    "        alpha = np.logspace(-3, 4, num=8)\n",
    "        train_score = []\n",
    "        CV_score = []\n",
    "        regs = []\n",
    "        \n",
    "        for a in alpha:\n",
    "            reg = Ridge(alpha = a)\n",
    "            reg.fit(X_train,y_train)\n",
    "            train_score.append(mean_squared_error(y_train,reg.predict(X_train)))\n",
    "            CV_score.append(mean_squared_error(y_CV,reg.predict(X_c)))\n",
    "            regs.append(reg)\n",
    "        \n",
    "        # find the best alpha in this fold\n",
    "        best_alpha = alpha[np.argmin(CV_score)]\n",
    "        \n",
    "        # grab the best model\n",
    "        reg = regs[np.argmin(CV_score)]\n",
    "        CV_scores.append(np.min(CV_score))\n",
    "        \n",
    "        # calculate test score using thee best model\n",
    "        test_scores.append(mean_squared_error(y_test,reg.predict(X_t)))\n",
    "    \n",
    "    return CV_scores,test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CV_scores, test_scores = ml_pipeline_kfold(X[:,np.newaxis],y,42,5)\n",
    "\n",
    "print('CV MSE:',np.around(np.mean(CV_scores),2),'+/-',np.around(np.std(CV_scores),2))\n",
    "print('test MSE:',np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some considerations\n",
    "- 1) lots of lines of code were written, mistakes can be easily made!\n",
    "- 2) kfold CV uses the same test set, so we do not estimate the uncertainty from random test sets\n",
    "   - test score uncertainty is lower than in the basic approach\n",
    "- 3) both approaches (basic and kfold) can fail if the data is imbalanced\n",
    "   - if one class is infrequent, it can happen that one set or one fold contains 0 points from the rare class\n",
    "   - sklearn will raise an error in that case\n",
    "- 4) neither of these approaches work, if data is not iid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using `GridSearchCV` and pipeline in k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def ml_pipeline_kfold(X,y,random_state,n_folds):\n",
    "    # create a test set\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)\n",
    "    \n",
    "    # splitter for _other\n",
    "    kf = KFold(n_splits=n_folds,shuffle=True,random_state=random_state)\n",
    "    \n",
    "    # create the pipeline: preprocessor + supervised ML method\n",
    "    scaler = StandardScaler()\n",
    "    pipe = make_pipeline(scaler,Ridge())\n",
    "    \n",
    "    # the parameter(s) we want to tune\n",
    "    param_grid = {'ridge__alpha': np.logspace(-3,4,num=8)}\n",
    "    \n",
    "    # prepare gridsearch\n",
    "    grid = GridSearchCV(pipe, \n",
    "                        param_grid = param_grid,\n",
    "                        scoring = make_scorer(mean_squared_error, greater_is_better=False),\n",
    "                        cv = kf, \n",
    "                        return_train_score = True)\n",
    "    \n",
    "    # do kfold CV on _other\n",
    "    grid.fit(X_other, y_other)\n",
    "    \n",
    "    return grid, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "grid, test_score = ml_pipeline_kfold(X[:,np.newaxis],y,42,5)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print('CV MSE:',-np.around(results[results['rank_test_score'] == 1]['mean_test_score'].values[0],2),\\\n",
    "      '+/-',np.around(results[results['rank_test_score'] == 1]['std_test_score'].values[0],2))\n",
    "print('test MSE:',-np.around(test_score,2))\n",
    "results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
