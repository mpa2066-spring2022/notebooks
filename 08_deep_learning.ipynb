{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736d11df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Very Brief Introduction to Deep Learning</h1></center>\n",
    "<center><h3>Paul Stey</h3></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1b43f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Impact of Deep Learning\n",
    "\n",
    "It would be difficult the overstate the impact.\n",
    "  * Facial recognition\n",
    "  * Image processing\n",
    "  * Voice recognition \n",
    "  * Medical imaging (e.g., tumor detection, pathology)\n",
    "  * Self-driving cars\n",
    "  * Game-playing AIs\n",
    "  * Virtual assistants (e.g., Siri, Alexa, Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea0fd5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Nomenclature\n",
    "\n",
    "Deep learning is a family modeling approaches with many names:\n",
    "\n",
    "  * Neural networks (NN)\n",
    "  * Deep neural networks (DNN)\n",
    "  * Artificial neural networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477a986",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "\n",
    "What is a neural network?\n",
    "  * Universal function approximator\n",
    "  * A species of directed acyclic graphs (usually)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e3a36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What do neural networks do?\n",
    "\n",
    "Like many other statistical or machine learning models (e.g., GLM, random forests, boosting), neural networks:\n",
    "  * Attempt to approximate a data-generating mechanism\n",
    "  * Can be used for classification problems\n",
    "  * Can be used for regression problems\n",
    "  * Can also be used for dimension reduction like principal components analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d31ef3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Networks vs. other ML Modeling\n",
    "\n",
    "Similarities to other types of machine learning models\n",
    "\n",
    "  * Input variables (i.e., _**X**_, features, predictors, etc.) and output variable (i.e., _y_)\n",
    "\n",
    "  \n",
    "<center><img src=\"images/input_output.png\" width=420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad3d08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications of Deep Learning\n",
    "Deep learning is extremely flexible, and can be applied to many domains.\n",
    "  \n",
    "<center><img src=\"images/self-driving_car.jpg\" width=860/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bd77d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History of Neural Networks\n",
    "\n",
    "The history of neural networks is long and somewhat tumultuous\n",
    "\n",
    "  * McCulloch and Pitts (1943) _A Logical Calculus of Ideas Immanent in Nervous Activity_\n",
    "  * Rosenblatt (1958) _The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3ea08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More Recently\n",
    "\n",
    "Neural networks are experiencing a major resurgence. There are at least three reasons.\n",
    "\n",
    "  * Better algorithms for back-propagation\n",
    "  * GPUs are well suited to building neural networks\n",
    "    - Matrix multiplies can be made embarrassingly parallel \n",
    "    - GPUs have much better memory bandwidth\n",
    "  * More labeled data\n",
    "  \n",
    "  \n",
    "<center><img src=\"images/two_johns.jpg\" width=380/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f0e35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "An early and fairly straightforward example of a neural network.\n",
    "\n",
    "<center><img src=\"images/neural_network3.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47434d55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Single Neuron\n",
    "\n",
    "A single neuron takes inputs, $x_j$, and applies the weights, $w_{\\cdot j}$ to the input by computing the dot product of the vectors $x$ and $w$. The result is the input to the \"activation function\".\n",
    "\n",
    "<center><img src=\"images/neuron2.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc9a66",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "Larger networks can have many, _many_ weights!\n",
    "  * Origin of the term \"deep\" neural networks \n",
    "  * Largest models have _trillions_ of weights (i.e., parameters)\n",
    "\n",
    "<center><img src=\"images/neural_network4.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904fb8c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Activation Functions\n",
    "  * The notion of an activation function comes again from the theoretical relationship to neurons in the brain.\n",
    "\n",
    "  * Activation functions are analogous to \"link\" functions in generalized linear models (GLMs). \n",
    "    \n",
    "  * In fact, one common activation function is the sigmoid function, which is just our old friend the logistic function which you are using when you fit logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbcfa29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Purpose of Activation Functions\n",
    "\n",
    "There are a few reasons we use activation functions.    \n",
    "\n",
    "  * Need to take some linear predictor and transform it so that it is bounded appropriate. For instance, the value of logistic function is in the range $(0, 1)$. \n",
    "  * Allows us to introduce non-linearities. \n",
    "    - Approximate a data-generating mechanism \n",
    "    - Trying to approximate a function that might be very complicated and include non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54750a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Activation Functions\n",
    "\n",
    "Some common activation functions include the following: \n",
    "  * Sigmoid (i.e., logistic)\n",
    "  * Hyperbolic tangent: $tanh$\n",
    "  * Rectified linear unit (ReLU)\n",
    "  * softplus\n",
    "  \n",
    "<center><img src=\"images/activation_functions.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405325a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Challenge Question</h1></center>\n",
    "\n",
    "The sigmoid and the ReLU activation functions are two of the most common in deep learning. The formulas for these are below. Write a `sigmoid()` and a `relu()` function in Python that implements these.\n",
    "\n",
    "$$s(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$r(x) = \\text{max}(0, x)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Hint:** Note that the NumPy module has the `e` constant included as a  part of the module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6da95",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Varieties of Neural Network (and layers)\n",
    "\n",
    "1. The \"feed-forward\" layer/network\n",
    "  * Mult-layer perceptron is a feed-foward network\n",
    "  * Most networks involve at least _some_ feed-foward layer\n",
    "2. Convolutional neural network (CNN)\n",
    "  * Ubiquitous in computer vision (i.e., image classification, object detection, facial recognition)\n",
    "3. Recurrent neural networks (RNN)\n",
    "  * Long short-term memory (LSTM) networks\n",
    "4. Generative adversarial network (GAN)\n",
    "  * Widely used in game-playing AI\n",
    "5. Autoencoders\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35cc85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "* Regular neural nets don't scale well to images\n",
    "  - For images of size $32 \\times 32 \\times 3$, a _single_ fully-connected neuron in the first layer would have $3072$ weights.\n",
    "  - Images of size $200 \\times 200 \\times 3$, a _single_ neuron gives $120000$ weights.\n",
    "* Full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e94f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CNNs (cont.)\n",
    "What are CNNs?\n",
    "  * ConvNets are very similar to neural networks discussed thus far. Dot product, followed by non-linearity, and loss function at the end.\n",
    "  * Explicit assumption that input are images.\n",
    "  * Layers have neurons arranged in 3 dimensions (width, height, depth) to form an **activation volume**\n",
    "  \n",
    "<center><img src=\"images/cnn.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccde69",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/convolution.gif\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289b197",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Orginal Image\n",
    "<center><img src=\"images/building.jpg\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cc2d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Apply Sobel operator filter\n",
    "\n",
    "<center><img src=\"images/building_sobel.jpg\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577e4d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Architecture of CNN\n",
    "\t\n",
    "\n",
    "Types of layers used to build ConvNets\n",
    "  * Convolutional Layer\n",
    "    - Input: 3-d volume\n",
    "    - Output: 3-d volume\n",
    "    - Convolutional \"filters\" with small regions in the image\n",
    "    - Output depth, depends on the number of filters\n",
    "  * Pooling Layer\n",
    "    - Downsampling along spatial dimensions (width, height)\n",
    "  * Fully-Connected Layer (what we've seen so far)\n",
    "    - Compute class score. Dimensions are transformed to $1 \\times 1 \\times k$, where $k$ is number of classes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbefaec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training vs. Inference\n",
    "\n",
    "1. Training neural network\n",
    "  * Process that computes weights (i.e., parameter estimates)\n",
    "  * Can take hours, days, weeks, or months\n",
    "  * Typically done on specialized hardware\n",
    "    - GPUs, TPUs, FPGAs\n",
    "2. Inference\n",
    "  * Use existing network (i.e., weights)\n",
    "  * Make predictions (i.e., classification, or numerical prediction)\n",
    "  * Happens fast; in many cases _extemely_ fast (e.g., milliseconds)\n",
    "  * Needs to happen on all kinds of devices (e.g., phones, cameras, sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed076d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning Packages\n",
    "\n",
    "1. TensorFlow\n",
    "  * Free, open-source software\n",
    "  * Primarily developed by Google\n",
    "  * C++ library, callable from C++, Python, or R\n",
    "2. Keras\n",
    "  * \"Front-end\" API for TensorFlow\n",
    "3. PyTorch\n",
    "  * Free, open-source software\n",
    "  * Developed in large part by Facebook\n",
    "  * C++/Cython library callable from Python\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
