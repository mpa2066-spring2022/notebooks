{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Tree-Based Models</h1></center>\n",
    "<center><h3>2022-03-22</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are \"Tree-based Methods\"?\n",
    "\n",
    "  * Species of regression/classification models\n",
    "  * These models are more \"algorithmic\", not so formulaic\n",
    "  * Basic idea is to build \"decision trees\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/amiahorse.jpg\" width=480/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Species of Tree-Based Methods\n",
    "\n",
    "  * ID3 (Quinlan, 1986)\n",
    "  * C4.5 (Quinlan, 1993)\n",
    "  * C5.0 (Quinlan, 1996)\n",
    "  * CART (Breiman, Friedman, Olshen, & Stone, 1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "  * Breiman _et al._ (1984)\n",
    "  * Tree built by recursive binary splits \n",
    "  * For regression, choose split that minimizes MSE\n",
    "  * For classification, choose splits by minimizing node impurity (e.g., Gini index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CART Algorithm Pseudo-code\n",
    "![image](images/cart_pseudo_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Titanic Example\n",
    "\n",
    "  * Titantic survival data\n",
    "  * Kaggle competition\n",
    "  * Predicting survival (2-class problem)\n",
    "  * Using demographic and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic_df = pd.read_csv(\"data/titanic_subset.csv\")\n",
    "\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Titanic CART Tree\n",
    "![image](images/tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advantages of CART Trees\n",
    "  * Intuitive\n",
    "  * Viable when $n \\ll p$\n",
    "  * Handle interactions naturally\n",
    "  * Minimal assumptions\n",
    "  * Handle missingness in predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Disadvantages of Single Trees\n",
    "  * High variance\n",
    "  * Prone to overfitting\n",
    "  * Lack of smoothness (troubling for regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bootstrap Aggregation (\"Bagging\")\n",
    "  * Breiman (1996)\n",
    "  * Extends idea of CART models\n",
    "    - Single trees overfit\n",
    "    - Stopping rules and pruning help, but only to a point\n",
    "  * Take $B$ bootstrap samples, build $B$ trees, aggregate predictions\n",
    "    - For regression, aggregation is taking the mean of the predicted values for each $y_i$\n",
    "\t- For classification, to aggregate means each tree casts a ``vote'' for each $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging Pseudo-Code\n",
    "![image](images/bagging_pseudo_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advantages of Bagging\n",
    "  * Big improvement in predictive performance\n",
    "  * Variance decreases\n",
    "  * Easy to tune\n",
    "  * Embarrassingly parallel\n",
    "  * Out-of-bag (OOB) error estimate (more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Challenge Problem</h1><center>\n",
    "    \n",
    "Let's write our own function for returning bootstrap samples. Let's call our function `bootstrap()`. It should take two arguments, `data` and `n_boot`; the first is a 1-dimmensional array, and the second is an integer. Our function should return a 2-dimmensional NumPy array with `n` rows and `n_boot` columns, where `n` is the length of `data` array, and where each column is a bootstrap sample from the origin `data`. \n",
    "    \n",
    "There are a few ways that we could do this, but we will almost certainly want to use the `np.random.choice()` function in NumPy to sample from a 1-D array.\n",
    "    \n",
    "**HINT**: Recall also that we can use the `np.zeros()` function to allocate an array of arbirary dimensions that's filled with zeros. This is frequently useful when we want to \"pre-allocated\" an array that we later fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define our function here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Testing our fuction\n",
    "x = np.random.normal(0, 1, 1_000_000)\n",
    "\n",
    "boot_samples = bootstrap(x, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Should print `True` 3 times\n",
    "print(boot_samples.shape == (1_000_000, 100))\n",
    "print(-0.1 < np.mean(boot_samples) < 0.1)\n",
    "print(0.9 < np.std(boot_samples) < 1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forests\n",
    "  * Ho (1995), Breiman (2001)}\n",
    "  * Extends idea of bagging}\n",
    "  * Build many trees, aggregate predictions}\n",
    "  * Only consider $m$ predictors at each node}\n",
    "  * Improve predictive performance}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests Pseudo-Code\n",
    "![image](images/random_forests_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Out-of-Bag (OOB) Samples\n",
    "  * For each bootstrap iteration, we have some $(y_i, \\bf{x_i})$  that weren't used in tree building\n",
    "  * Use these $(y_i, \\bf{x}_i)$ OOB samples to estimate test error\n",
    "  * Also use these $(y_i, \\bf{x_i})$ to generate measures of variable importance (more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## OOB and Test Error\n",
    "![image](images/oob_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests and Overfitting\n",
    "\n",
    "  * It was once (mistakenly) believed that random forests would not over fit\n",
    "  * You _can_ overfit by growing deep trees\n",
    "  * Growing too many trees won't cause you to overfit (but it's wasteful of computing resources)\n",
    "  * In general, people worry _much_ less about overfitting in random forests (relative to other approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fitting Random Forest\n",
    "\n",
    "Fitting random forests models using Python is fairly straightforward. There is an excellent implementation of random forests in the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Data\n",
    "* Handwritten digits\n",
    "* Ten-class classification problem (i.e., 0-9 classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version = 1, return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# quick function to plot image from MNIST\n",
    "\n",
    "def show_image(x):\n",
    "    x_resize = np.array(x).reshape(28, 28)\n",
    "    plt.imshow(x_resize, \n",
    "               cmap = \"Blues\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 137                    # image number\n",
    "show_image(X.iloc[n])      # plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Split Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting our Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create model object\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs = 2, \n",
    "                             n_estimators = 100,\n",
    "                             oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)     # fit our model (takes time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rfc.oob_score_               # Out-of-bag error estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rfc.score(X_test, y_test)    # test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrast with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mod = LogisticRegression(multi_class = \"multinomial\", max_iter = 5000)       # create model object\n",
    "\n",
    "mod.fit(X_train, y_train)               # fit model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = mod.predict(X_test)            # use fitted model to make predictions\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
